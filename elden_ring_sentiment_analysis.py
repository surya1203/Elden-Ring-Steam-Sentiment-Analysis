# -*- coding: utf-8 -*-
"""Elden Ring Sentiment Analysis.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1RQKgPiJ0hu3uOO8-dbqT17-pbNnASHbX

# Import Libraries
"""

!pip install transformers
!pip install lxml
!pip install nlpaug
from imblearn.over_sampling import SMOTE
import requests
from bs4 import BeautifulSoup
import os
import json
import pandas as pd
from sklearn.model_selection import train_test_split
from transformers import DistilBertTokenizer, TFDistilBertForSequenceClassification
import tensorflow as tf
import seaborn as sns
import matplotlib.pyplot as plt
import requests
from bs4 import BeautifulSoup
from lxml import etree
from imblearn.over_sampling import RandomOverSampler
from imblearn.under_sampling import RandomUnderSampler
import nltk
from nltk.corpus import stopwords
from nltk.stem import WordNetLemmatizer
from nltk.tokenize import word_tokenize
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.linear_model import LogisticRegression
import nlpaug.augmenter.word as naw
from sklearn.metrics import confusion_matrix

"""# EDA"""

# Load the dataset
df = pd.read_csv("elden_ring_steam_reviews.csv")

print(df.info())

print(df.head())

df.describe()

df.corrwith(df.voted_up)

# # Calculate the count of each class in the 'voted_up' column
# class_counts = df['voted_up'].value_counts()

#@title
# # Plot a bar graph
# plt.bar(class_counts.index, class_counts.values)

# # Add labels and title to the plot
# plt.xlabel('voted_up')
# plt.ylabel('Count')
# plt.title('Distribution of voted_up')

# # Add value annotations above each bar
# for i, v in enumerate(class_counts.values):
#     plt.text(i, v + 10, str(v), ha='center')

# # Show the plot
# plt.show()

# Count the number of true and false values
true_count = df[df["voted_up"] == True].shape[0]
false_count = df[df["voted_up"] == False].shape[0]

print("Number of Positive Reviews: ", true_count)
print("Number of Negative Reviews: ", false_count)

print("----------------------------------------------------------------------------")

# Create a bar plot
plt.bar(["True", "False"], [true_count, false_count])

# Set plot title and axis labels
plt.title("Number of True vs False Values")
plt.xlabel("Values")
plt.ylabel("Count")

# Show the plot
plt.show()

# Make a copy of the DataFrame
df_corr = df.copy()

# Drop the "voted_up" column from the copy
df_corr = df_corr.drop("voted_up", axis=1)

# Filter out non-numeric columns
numeric_columns = df_corr.select_dtypes(include=['float64', 'int64']).columns

# Calculate correlation with "voted_up" column for numeric columns only
correlation = df_corr[numeric_columns].apply(lambda x: x.corr(df['voted_up']))

# Sort correlation values in descending order
correlation = correlation.sort_values(ascending=False)

# Print the correlation results
print(correlation)

"""The more the number of games a person owns the review given by the person for the game dips. It shows that a person becomes more critical about games when he/she plays more games.    """

# Plot the correlation values
plt.bar(correlation.index, correlation.values)

# Add labels and title to the plot
plt.xlabel('Numeric Columns')
plt.ylabel('Correlation with voted_up')
plt.title('Correlation between Numeric Columns and voted_up')

# Rotate x-axis labels for better visibility
plt.xticks(rotation=90)

# Show the plot
plt.show()

"""# Pre-processing"""

# Convert the "review" column to strings
df["review"] = df["review"].astype(str)

# Remove unnecessary columns
df = df.drop(columns=["id", "created", "votes_up", "language","comment_count", "steam_purchase",
                      "recieved_for_free", "written_during_early_access", "author_num_games_owned",
                      "author_num_reviews", "author_playtime_forever", "author_playtime_last_two_weeks",
                      "author_last_played", "author_playtime_at_review"])
df

# Convert 'voted_up' column to boolean. EDIT: No need, already in boolean
# df['voted_up'] = df['voted_up'].map(lambda x: True if x == 'TRUE' else False)

# Split the data into features (reviews) and labels (voted_up)
reviews = df['review'].values
labels = df['voted_up'].values

#@title
# labels

#@title
# get_indexes = lambda x, xs: [i for (y, i) in zip(xs, range(len(xs))) if x == y]

#@title
# get_indexes(True,labels)

# Split the data into training and testing sets
train_reviews, test_reviews, train_labels, test_labels = train_test_split(
    reviews, labels, test_size=0.2, random_state=42)

# Further split the training set into training and validation sets
train_reviews, val_reviews, train_labels, val_labels = train_test_split(
    train_reviews, train_labels, test_size=0.2, random_state=42)

train_reviews

train_labels

test_reviews

test_labels

val_reviews

val_labels

"""# Sentiment Analysis

## Tokenization
"""

# Load the DistilBERT tokenizer
tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')

# Tokenize the training reviews
train_encodings = tokenizer.batch_encode_plus(
    train_reviews.tolist(),
    truncation=True,
    padding=True,
    return_token_type_ids=False,
    max_length=512,
    return_attention_mask=True,
    return_tensors='tf'
)

# Tokenize the validation reviews
val_encodings = tokenizer.batch_encode_plus(
    val_reviews.tolist(),
    truncation=True,
    padding=True,
    return_token_type_ids=False,
    max_length=512,
    return_attention_mask=True,
    return_tensors='tf'
)

# Tokenize the testing reviews
test_encodings = tokenizer.batch_encode_plus(
    test_reviews.tolist(),
    truncation=True,
    padding=True,
    return_token_type_ids=False,
    max_length=512,
    return_attention_mask=True,
    return_tensors='tf'
)

# Create TensorFlow datasets from the tokenized data
train_dataset = tf.data.Dataset.from_tensor_slices((
    {
        'input_ids': train_encodings['input_ids'],
        'attention_mask': train_encodings['attention_mask']
    },
    train_labels
))

val_dataset = tf.data.Dataset.from_tensor_slices((
    {
        'input_ids': val_encodings['input_ids'],
        'attention_mask': val_encodings['attention_mask']
    },
    val_labels
))

test_dataset = tf.data.Dataset.from_tensor_slices((
    {
        'input_ids': test_encodings['input_ids'],
        'attention_mask': test_encodings['attention_mask']
    },
    test_labels
))

"""# DistilBERT Model fitting


"""

# Load the pre-trained DistilBERT model
model = TFDistilBertForSequenceClassification.from_pretrained('distilbert-base-uncased')

# Compile the model. We can tweak the hyperparameters later
model.compile(
    optimizer=tf.keras.optimizers.Adam(learning_rate=2e-5),
    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),
    metrics=[tf.keras.metrics.SparseCategoricalAccuracy('accuracy')]
)

# Train the model
history = model.fit(
    train_dataset.shuffle(100).batch(16),
    epochs=3,
    batch_size=16,
    validation_data=val_dataset.shuffle(100).batch(16),
    verbose=1
)

"""# Evaluation"""

from sklearn.metrics import f1_score

# Evaluate the model on the test dataset
results = model.evaluate(test_dataset.batch(16), verbose=0)
accuracy = results[1]

# Get accuracy and F1 score
predictions = model.predict(test_dataset.batch(16)).logits.argmax(axis=-1)
f1 = f1_score(test_labels, predictions)

print('Accuracy:', accuracy)
print('F1 score:', f1)

#Classification metrics
from sklearn.metrics import classification_report

metrics = classification_report(test_labels, predictions)

print(metrics)

"""# Balancing the minority class"""

df.to_csv('df.csv',index=False)

# Balance the minority class using data augmentation
augmenter = naw.SynonymAug(aug_src='wordnet')
minority_class = df[df['voted_up'] == False]
augmented_reviews = []
for index, row in minority_class.iterrows():
  review = str(row['review'])  # Convert to string
  augmented_reviews.extend(augmenter.augment(row['review'], n=1))

augmented_reviews

augmented_data = pd.DataFrame({
    'review': augmented_reviews,
    'voted_up': [False] * len(augmented_reviews)
})

augmented_data['review']

balanced_data = pd.concat([df, augmented_data], ignore_index=True)

balanced_data['review']

# Split the data into features (reviews) and labels (voted_up)
reviews_bal = balanced_data['review'].values
labels_bal = balanced_data['voted_up'].values

reviews_bal

# Split the data into training and testing sets
train_reviews, test_reviews, train_labels, test_labels = train_test_split(
    reviews_bal, labels_bal, test_size=0.2, random_state=42)

# Further split the training set into training and validation sets
train_reviews, val_reviews, train_labels, val_labels = train_test_split(
    train_reviews, train_labels, test_size=0.2, random_state=42)

# Load the DistilBERT tokenizer
tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')

# Tokenize the training reviews
train_encodings = tokenizer.batch_encode_plus(
    train_reviews.tolist(),
    truncation=True,
    padding=True,
    return_token_type_ids=False,
    max_length=512,
    return_attention_mask=True,
    return_tensors='tf'
)

# Tokenize the validation reviews
val_encodings = tokenizer.batch_encode_plus(
    val_reviews.tolist(),
    truncation=True,
    padding=True,
    return_token_type_ids=False,
    max_length=512,
    return_attention_mask=True,
    return_tensors='tf'
)

# Tokenize the testing reviews
test_encodings = tokenizer.batch_encode_plus(
    test_reviews.tolist(),
    truncation=True,
    padding=True,
    return_token_type_ids=False,
    max_length=512,
    return_attention_mask=True,
    return_tensors='tf'
)

# Create TensorFlow datasets from the tokenized data
train_dataset = tf.data.Dataset.from_tensor_slices((
    {
        'input_ids': train_encodings['input_ids'],
        'attention_mask': train_encodings['attention_mask']
    },
    train_labels
))

val_dataset = tf.data.Dataset.from_tensor_slices((
    {
        'input_ids': val_encodings['input_ids'],
        'attention_mask': val_encodings['attention_mask']
    },
    val_labels
))

test_dataset = tf.data.Dataset.from_tensor_slices((
    {
        'input_ids': test_encodings['input_ids'],
        'attention_mask': test_encodings['attention_mask']
    },
    test_labels
))

# Load the pre-trained DistilBERT model
model = TFDistilBertForSequenceClassification.from_pretrained('distilbert-base-uncased')

# Compile the model. We can tweak the hyperparameters later
model.compile(
    optimizer=tf.keras.optimizers.Adam(learning_rate=2e-5),
    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),
    metrics=[tf.keras.metrics.SparseCategoricalAccuracy('accuracy')]
)

# Train the model
history = model.fit(
    train_dataset.shuffle(100).batch(16),
    epochs=3,
    batch_size=16,
    validation_data=val_dataset.shuffle(100).batch(16),
    verbose=1
)

"""# Evaluation"""

from sklearn.metrics import f1_score

# Evaluate the model on the test dataset
results = model.evaluate(test_dataset.batch(16), verbose=0)
accuracy = results[1]

# Get accuracy and F1 score
predictions = model.predict(test_dataset.batch(16)).logits.argmax(axis=-1)
f1 = f1_score(test_labels, predictions)

print('Accuracy:', accuracy)
print('F1 score:', f1)

#Classification metrics
from sklearn.metrics import classification_report

metrics = classification_report(test_labels, predictions)

print(metrics)

# Calculate the confusion matrix
cm = confusion_matrix(test_labels, predictions)

# Plot the confusion matrix
plt.figure(figsize=(8, 6))
sns.heatmap(cm, annot=True, fmt="d", cmap="Blues")
plt.title("Confusion Matrix")
plt.xlabel("Predicted Labels")
plt.ylabel("True Labels")
plt.show()

# Plot a bar plot to visualize the distribution of predicted labels
predicted_counts = pd.Series(predictions).value_counts()
plt.figure(figsize=(6, 4))
# sns.barplot(predicted_counts.index, predicted_counts.values)
plt.title("Distribution of Predicted Labels")
plt.xlabel("Labels")
plt.ylabel("Count")
plt.show()